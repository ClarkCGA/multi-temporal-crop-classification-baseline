{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f6753-1db1-4cbd-b141-e2c680564455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, time, math, random, numbers, itertools, tqdm, importlib, re\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rasterio\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from sklearn import metrics\n",
    "from skimage import transform as trans\n",
    "from pathlib import Path\n",
    "from collections.abc import Sequence\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.ndimage import rotate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056708e-0e49-4e4d-9d75-b65b250b791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code cell is used to add the src directory to the Python path, making \n",
    "# it possible to import modules from that directory. \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba94f1-2576-4bcc-bf6e-ce69f837f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset import CropData\n",
    "from models.unet import Unet\n",
    "from model_compiler import ModelCompiler\n",
    "from custom_loss_functions import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5f7b9-5c90-4218-88d2-81f0b685f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code cell loads a configuration file (default_config.yaml) using the YAML library and stores the \n",
    "# configuration data in the config dictionary. Then, it processes the global_stats section of the config \n",
    "# dictionary by expanding the lists for each stats based on the number of available time points. \n",
    "# As you can see we decided to generate a single set of normalization statistics and use it to \n",
    "# normalize all the time-points.\n",
    "\n",
    "yaml_config_path = \"/home/workdir/config/default_config.yaml\"  # replace this path to your own config file.\n",
    "num_time_points = 3  # Change this number accordingly if you use a dataset with a different temporal length.\n",
    "\n",
    "with open(yaml_config_path, 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "\n",
    "# Perform multiplication and concatenation for each key in global_stats\n",
    "for key, value in config['global_stats'].items():\n",
    "    config['global_stats'][key] = value * num_time_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8fdf3-da45-4e08-b274-b243bd2a76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# pretty-print the config dictionary\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(config, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f12c3-a3d7-466e-8f4d-3978cd90052a",
   "metadata": {},
   "source": [
    "### Steps for training and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00efdb-8a76-4271-b77d-fb95faac315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-1: Data Preparation and Loading\n",
    "# This code cell reads the input dataset for training, pre-processes it, and creates a 'torch.utils.Dataset' \n",
    "# object to be used in the model training process. It utilizes the 'CropData' class from the custom module \n",
    "# (CropData.py) to achieve this.\n",
    "\n",
    "train_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                         usage=\"train\",\n",
    "                         dataset_name=config[\"train_dataset_name\"],\n",
    "                         csv_path=config[\"train_csv_path\"],\n",
    "                         apply_normalization=config[\"apply_normalization\"],\n",
    "                         normal_strategy=config[\"normal_strategy\"],\n",
    "                         stat_procedure=config[\"stat_procedure\"],\n",
    "                         global_stats=config[\"global_stats\"],\n",
    "                         trans=config[\"transformations\"], \n",
    "                         **config[\"aug_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94a8cf-a055-4dba-b0d6-bb5f82d7e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "\n",
    "show_random_patches(train_dataset, sample_num=3, rgb_bands=(3, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf4d90-8398-4ea0-8920-d76f710b051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "\n",
    "labels_count = get_labels_distribution(train_dataset, num_classes=14, ignore_class=0)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b898a60-29f3-4ec8-99fb-dee3bdabfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-2: Data Batching and Device Assignment\n",
    "# This code cell batchifies the training dataset using the 'DataLoader' class from PyTorch. The 'DataLoader' \n",
    "# efficiently loads the 'train_dataset' in batches, facilitating memory management during training.\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config[\"train_BatchSize\"], \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec53bd-77b4-4282-b062-66928cdaca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-3\n",
    "# Read the input dataset for validation, pre-process it and generate a 'torch.utils.Dataset' object \n",
    "# that can be accepted to get loaded into the model.\n",
    "\n",
    "val_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                       usage=\"validation\",\n",
    "                       dataset_name=config[\"train_dataset_name\"],\n",
    "                       csv_path=config[\"val_csv_path\"],\n",
    "                       apply_normalization=config[\"apply_normalization\"],\n",
    "                       normal_strategy=config[\"normal_strategy\"],\n",
    "                       stat_procedure=config[\"stat_procedure\"],\n",
    "                       global_stats=config[\"global_stats\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b87734-63bc-4495-b63a-b9af626d4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "\n",
    "labels_count = get_labels_distribution(val_dataset)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9432a-dbf3-4981-ae2c-144aa5f46ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-4\n",
    "# Batchify the validation dataset and put it on the defined 'Device'.\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46523fb6-c853-48c4-b671-1adc41997759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Model Initialization\n",
    "# The model is based on the U-Net architecture, a popular choice for image segmentation tasks. \n",
    "\n",
    "model = Unet(n_classes=config[\"n_classes\"], \n",
    "             in_channels=config[\"input_channels\"], \n",
    "             use_skipAtt=config[\"use_skipAtt\"],\n",
    "             filter_config=config[\"filter_config\"],\n",
    "             dropout_rate=config[\"train_dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b69356-5b9d-4004-8db7-0ea187e3d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Model Compilation and Training Setup\n",
    "# This code cell compiles the deep learning model, making it ready for training and evaluation. The 'ModelCompiler' \n",
    "# class is employed for this purpose, which offers a set of essential functionalities, such as model training \n",
    "# ('fit'), model saving, accuracy evaluation ('accuracy_evaluation'), inference ('inference'), and checkpointing \n",
    "# with resume capabilities. Additionally, it generates tensorboard reports and graphs for monitoring the training \n",
    "# process.\n",
    "\n",
    "# Note: For training from scratch pass the \"params_init\" and \"freeze_params\" as None which results in loading a vanilla\n",
    "# model with random initialization based on the \"model_init_type\" parameter. For training with warmup, only change the\n",
    "# \"params_init\" to the path to model's saved parameter (not a checkpoint). For fine-tuning, change the \"params_init\" to \n",
    "# the path to model's saved parameter (by default its in the chckpt folder in working directory) and pass a list of integers\n",
    "# representing the index of layers to be frozen.\n",
    "\n",
    "compiled_model = ModelCompiler(model,\n",
    "                               working_dir=config[\"working_dir\"],\n",
    "                               out_dir=config[\"out_dir\"],\n",
    "                               num_classes=config[\"n_classes\"],\n",
    "                               inch=config[\"input_channels\"],\n",
    "                               class_mapping=config[\"class_mapping\"],\n",
    "                               gpu_devices=config[\"gpuDevices\"],\n",
    "                               model_init_type=config[\"init_type\"], \n",
    "                               params_init=config[\"params_init\"],\n",
    "                               freeze_params=config[\"freeze_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c960-0171-4f7a-a678-82dc11c75f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Model Training and Validation\n",
    "# This code cell trains and validates the compiled deep learning model for the specified number of epochs. \n",
    "# The model is trained using the \"fit\" method provided by the 'compiled_model' object. The training data \n",
    "# is obtained from 'train_loader', which contains batched samples and labels. Additionally, the 'val_loader' \n",
    "# is used for model validation during training. For each epoch the model reports the average loss value for both \n",
    "# training and validation phases, providing insights into its generalization capabilities.\n",
    "\n",
    "# During the training the \"fit\" method will create a folder called 'chkpt' inside your working directory\n",
    "# and save the checkpoints in user-defined intervals in that folder.\n",
    "# Note: In case the training process is interrupted, you can change the \"resume\" argument and pass in the\n",
    "# epoch to resume based on the created checkpoints.\n",
    "\n",
    "criterion_name = config['criterion']['name']\n",
    "weight = config['criterion']['weight']\n",
    "ignore_index = config['criterion']['ignore_index']\n",
    "gamma = config['criterion']['gamma']\n",
    "\n",
    "if criterion_name == 'TverskyFocalLoss':\n",
    "    criterion = TverskyFocalLoss(weight=weight, ignore_index=ignore_index, gamma=gamma)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index)\n",
    "    \n",
    "\n",
    "compiled_model.fit(train_loader,\n",
    "                   val_loader, \n",
    "                   epochs=config[\"epochs\"], \n",
    "                   optimizer_name=config[\"optimizer\"], \n",
    "                   lr_init=config[\"LR\"],\n",
    "                   lr_policy=config[\"LR_policy\"], \n",
    "                   criterion=criterion, \n",
    "                   momentum=config[\"momentum\"],\n",
    "                   checkpoint_interval=config[\"checkpoint_interval\"],\n",
    "                   resume=config[\"resume\"],\n",
    "                   resume_epoch=config[\"resume_epoch\"],\n",
    "                   **config[\"lr_prams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128e11f-149e-4a47-a4d4-9f31880b08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5\n",
    "# Same model parameters\n",
    "compiled_model.save(save_object=\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3f31e-6976-4e2e-a659-c03e636e630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6\n",
    "# Generate the accuracy metrics.\n",
    "metrics = compiled_model.accuracy_evaluation(val_loader, filename=config[\"val_metric_fname\"], \n",
    "                                             unknown_class_idx=config[\"unknown_class_idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c97ced-b4b0-476a-9684-a52a4568f25d",
   "metadata": {},
   "source": [
    "### Steps for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941320c7-ebee-4ce5-b3b0-3ce3c938b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                       usage=\"inference\",\n",
    "                       dataset_name=config[\"train_dataset_name\"],\n",
    "                       csv_path=config[\"test_csv_path\"],\n",
    "                       apply_normalization=config[\"apply_normalization\"],\n",
    "                       normal_strategy=config[\"normal_strategy\"],\n",
    "                       stat_procedure=config[\"stat_procedure\"],\n",
    "                       global_stats=config[\"global_stats\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb8956-4b12-4fa6-a2ee-3fa70858c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_handling_collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_ids = []\n",
    "    img_metas = []\n",
    "\n",
    "    # Unpack elements from each sample in the batch\n",
    "    for sample in batch:\n",
    "        images.append(sample[0])\n",
    "        labels.append(sample[1])\n",
    "        img_ids.append(sample[2])\n",
    "        img_metas.append(sample[3])  # append the dict to the list\n",
    "\n",
    "    # Stack images and labels into a single tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    \n",
    "    return images, labels, img_ids, img_metas\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False,\n",
    "                        collate_fn=meta_handling_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c16e40-8dc4-46ad-a4f6-434ee9c50395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(n_classes=config[\"n_classes\"], \n",
    "             in_channels=config[\"input_channels\"], \n",
    "             use_skipAtt=config[\"use_skipAtt\"],\n",
    "             filter_config=config[\"filter_config\"],\n",
    "             dropout_rate=config[\"train_dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4159851-b338-47ef-b172-0554c0659eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ModelCompiler(model,\n",
    "                               working_dir=config[\"working_dir\"],\n",
    "                               out_dir=config[\"out_dir\"],\n",
    "                               num_classes=config[\"n_classes\"],\n",
    "                               inch=config[\"input_channels\"],\n",
    "                               class_mapping=config[\"class_mapping\"],\n",
    "                               gpu_devices=config[\"gpuDevices\"],\n",
    "                               model_init_type=config[\"init_type\"], \n",
    "                               params_init=config[\"params_init\"],\n",
    "                               freeze_params=config[\"freeze_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521d916-d09c-4720-9727-f26c9cba0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model.inference(test_loader, out_dir=config[\"out_dir\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
