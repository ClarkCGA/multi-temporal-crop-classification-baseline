{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f6753-1db1-4cbd-b141-e2c680564455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, time, math, random, numbers, itertools, tqdm, importlib, re\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rasterio\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from sklearn import metrics\n",
    "from skimage import transform as trans\n",
    "from pathlib import Path\n",
    "from collections.abc import Sequence\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.ndimage import rotate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056708e-0e49-4e4d-9d75-b65b250b791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code cell is used to add the src directory to the Python path, making \n",
    "# it possible to import modules from that directory. \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba94f1-2576-4bcc-bf6e-ce69f837f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset import CropData\n",
    "from models.unet import Unet\n",
    "from model_compiler import ModelCompiler\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5f7b9-5c90-4218-88d2-81f0b685f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code cell loads a configuration file (default_config.yaml) using the YAML library and stores the \n",
    "# configuration data in the config dictionary. Then, it processes the global_stats section of the config \n",
    "# dictionary by expanding the lists for each stats based on the number of available time points. \n",
    "# As you can see we decided to generate a single set of normalization statistics and use it to \n",
    "# normalize all the time-points.\n",
    "\n",
    "yaml_config_path = \"/home/workdir/config/default_config.yaml\"  # replace this path to your own config file.\n",
    "num_time_points = 3  # Change this number accordingly if you use a dataset with a different temporal length.\n",
    "\n",
    "with open(yaml_config_path, 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "\n",
    "# Perform multiplication and concatenation for each key in global_stats\n",
    "for key, value in config['global_stats'].items():\n",
    "    config['global_stats'][key] = value * num_time_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8fdf3-da45-4e08-b274-b243bd2a76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# pretty-print the config dictionary\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(config, width=100, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48349d96-0920-475c-9951-dd7196d847cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['criterion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f12c3-a3d7-466e-8f4d-3978cd90052a",
   "metadata": {},
   "source": [
    "### Steps for training and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00efdb-8a76-4271-b77d-fb95faac315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-1: Data Preparation and Loading\n",
    "# This code cell reads the input dataset for training, pre-processes it, and creates a 'torch.utils.Dataset' \n",
    "# object to be used in the model training process. It utilizes the 'CropData' class from the custom module \n",
    "# (CropData.py) to achieve this.\n",
    "\n",
    "train_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                         usage=\"train\",\n",
    "                         dataset_name=config[\"train_dataset_name\"],\n",
    "                         csv_path=config[\"train_csv_path\"],\n",
    "                         apply_normalization=config[\"apply_normalization\"],\n",
    "                         normal_strategy=config[\"normal_strategy\"],\n",
    "                         stat_procedure=config[\"stat_procedure\"],\n",
    "                         global_stats=config[\"global_stats\"],\n",
    "                         trans=config[\"transformations\"], \n",
    "                         **config[\"aug_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94a8cf-a055-4dba-b0d6-bb5f82d7e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "\n",
    "show_random_patches(train_dataset, sample_num=3, rgb_bands=(3, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf4d90-8398-4ea0-8920-d76f710b051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "\n",
    "labels_count = get_labels_distribution(train_dataset, num_classes=14, ignore_class=0)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b898a60-29f3-4ec8-99fb-dee3bdabfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-2: Data Batching and Device Assignment\n",
    "# This code cell batchifies the training dataset using the 'DataLoader' class from PyTorch. The 'DataLoader' \n",
    "# efficiently loads the 'train_dataset' in batches, facilitating memory management during training.\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config[\"train_BatchSize\"], \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec53bd-77b4-4282-b062-66928cdaca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-3\n",
    "# Read the input dataset for validation, pre-process it and generate a 'torch.utils.Dataset' object \n",
    "# that can be accepted to get loaded into the model.\n",
    "\n",
    "val_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                       usage=\"validation\",\n",
    "                       dataset_name=config[\"train_dataset_name\"],\n",
    "                       csv_path=config[\"val_csv_path\"],\n",
    "                       apply_normalization=config[\"apply_normalization\"],\n",
    "                       normal_strategy=config[\"normal_strategy\"],\n",
    "                       stat_procedure=config[\"stat_procedure\"],\n",
    "                       global_stats=config[\"global_stats\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b87734-63bc-4495-b63a-b9af626d4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "\n",
    "labels_count = get_labels_distribution(val_dataset)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9432a-dbf3-4981-ae2c-144aa5f46ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-4\n",
    "# Batchify the validation dataset and put it on the defined 'Device'.\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46523fb6-c853-48c4-b671-1adc41997759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Model Initialization\n",
    "# The model is based on the U-Net architecture, a popular choice for image segmentation tasks. \n",
    "\n",
    "model = Unet(n_classes=config[\"n_classes\"], \n",
    "             in_channels=config[\"input_channels\"], \n",
    "             use_skipAtt=config[\"use_skipAtt\"],\n",
    "             filter_config=config[\"filter_config\"],\n",
    "             dropout_rate=config[\"train_dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b69356-5b9d-4004-8db7-0ea187e3d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Model Compilation and Training Setup\n",
    "# This code cell compiles the deep learning model, making it ready for training and evaluation. The 'ModelCompiler' \n",
    "# class is employed for this purpose, which offers a set of essential functionalities, such as model training \n",
    "# ('fit'), model saving, accuracy evaluation ('accuracy_evaluation'), inference ('inference'), and checkpointing \n",
    "# with resume capabilities. Additionally, it generates tensorboard reports and graphs for monitoring the training \n",
    "# process.\n",
    "\n",
    "# Note: For training from scratch pass the \"params_init\" and \"freeze_params\" as None which results in loading a vanilla\n",
    "# model with random initialization based on the \"model_init_type\" parameter. For training with warmup, only change the\n",
    "# \"params_init\" to the path to model's saved parameter (not a checkpoint). For fine-tuning, change the \"params_init\" to \n",
    "# the path to model's saved parameter (by default its in the chckpt folder in working directory) and pass a list of integers\n",
    "# representing the index of layers to be frozen.\n",
    "\n",
    "compiled_model = ModelCompiler(model,\n",
    "                               working_dir=config[\"working_dir\"],\n",
    "                               out_dir=config[\"out_dir\"],\n",
    "                               num_classes=config[\"n_classes\"],\n",
    "                               inch=config[\"input_channels\"],\n",
    "                               class_mapping=config[\"class_mapping\"],\n",
    "                               gpu_devices=config[\"gpuDevices\"],\n",
    "                               model_init_type=config[\"init_type\"], \n",
    "                               params_init=config[\"params_init\"],\n",
    "                               freeze_params=config[\"freeze_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c960-0171-4f7a-a678-82dc11c75f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Model Training and Validation\n",
    "# This code cell trains and validates the compiled deep learning model for the specified number of epochs. \n",
    "# The model is trained using the \"fit\" method provided by the 'compiled_model' object. The training data \n",
    "# is obtained from 'train_loader', which contains batched samples and labels. Additionally, the 'val_loader' \n",
    "# is used for model validation during training. For each epoch the model reports the average loss value for both \n",
    "# training and validation phases, providing insights into its generalization capabilities.\n",
    "\n",
    "# During the training the \"fit\" method will create a folder called 'chkpt' inside your working directory\n",
    "# and save the checkpoints in user-defined intervals in that folder.\n",
    "# Note: In case the training process is interrupted, you can change the \"resume\" argument and pass in the\n",
    "# epoch to resume based on the created checkpoints.\n",
    "\n",
    "criterion_name = config['criterion']['name']\n",
    "weight = config['criterion']['weight']\n",
    "ignore_index = config['criterion']['ignore_index']\n",
    "gamma = config['criterion']['gamma']\n",
    "\n",
    "if criterion_name == 'TverskyFocalLoss':\n",
    "    criterion = TverskyFocalLoss(weight=weight, ignore_index=ignore_index, gamma=gamma)\n",
    "else:\n",
    "    criterion = TverskyFocalLoss(weight=weight, ignore_index=ignore_index)\n",
    "    \n",
    "\n",
    "compiled_model.fit(train_loader,\n",
    "                   val_loader, \n",
    "                   epochs=config[\"epochs\"], \n",
    "                   optimizer_name=config[\"optimizer\"], \n",
    "                   lr_init=config[\"LR\"],\n",
    "                   lr_policy=config[\"LR_policy\"], \n",
    "                   criterion=criterion, \n",
    "                   momentum=config[\"momentum\"],\n",
    "                   checkpoint_interval=config[\"checkpoint_interval\"],\n",
    "                   resume=config[\"resume\"],\n",
    "                   resume_epoch=config[\"resume_epoch\"],\n",
    "                   **config[\"lr_prams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128e11f-149e-4a47-a4d4-9f31880b08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5\n",
    "# Same model parameters\n",
    "compiled_model.save(save_object=\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3f31e-6976-4e2e-a659-c03e636e630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6\n",
    "# Generate the accuracy metrics.\n",
    "metrics = compiled_model.accuracy_evaluation(val_loader, filename=config[\"val_metric_fname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c97ced-b4b0-476a-9684-a52a4568f25d",
   "metadata": {},
   "source": [
    "### Steps for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941320c7-ebee-4ce5-b3b0-3ce3c938b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                       usage=\"inference\",\n",
    "                       dataset_name=config[\"train_dataset_name\"],\n",
    "                       csv_path=config[\"test_csv_path\"],\n",
    "                       apply_normalization=config[\"apply_normalization\"],\n",
    "                       normal_strategy=config[\"normal_strategy\"],\n",
    "                       stat_procedure=config[\"stat_procedure\"],\n",
    "                       global_stats=config[\"global_stats\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb8956-4b12-4fa6-a2ee-3fa70858c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_handling_collate_fn(batch):\n",
    "    images = []\n",
    "    labels = []\n",
    "    img_ids = []\n",
    "    img_metas = []\n",
    "\n",
    "    # Unpack elements from each sample in the batch\n",
    "    for sample in batch:\n",
    "        images.append(sample[0])\n",
    "        labels.append(sample[1])\n",
    "        img_ids.append(sample[2])\n",
    "        img_metas.append(sample[3])  # append the dict to the list\n",
    "\n",
    "    # Stack images and labels into a single tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    \n",
    "    return images, labels, img_ids, img_metas\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False,\n",
    "                        collate_fn=meta_handling_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e13a1-18f5-46ba-b0a8-3f5cd22a3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels, img_ids, img_metas = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed26f33-a671-450d-8c6e-ce9bc59ae975",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543a192-8c93-4799-8762-395e1e40ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas[\"transform\"] = [t.item() for t in img_metas[\"transform\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b41b27-6300-40ef-bb21-c21361dbb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = img_metas[\"transform\"]\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c16e40-8dc4-46ad-a4f6-434ee9c50395",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(n_classes=config[\"n_classes\"], \n",
    "             in_channels=config[\"input_channels\"], \n",
    "             use_skipAtt=config[\"use_skipAtt\"],\n",
    "             filter_config=config[\"filter_config\"],\n",
    "             dropout_rate=config[\"train_dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4159851-b338-47ef-b172-0554c0659eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ModelCompiler(model,\n",
    "                               working_dir=config[\"working_dir\"],\n",
    "                               out_dir=config[\"out_dir\"],\n",
    "                               num_classes=config[\"n_classes\"],\n",
    "                               inch=config[\"input_channels\"],\n",
    "                               class_mapping=config[\"class_mapping\"],\n",
    "                               gpu_devices=config[\"gpuDevices\"],\n",
    "                               model_init_type=config[\"init_type\"], \n",
    "                               params_init=config[\"params_init\"],\n",
    "                               freeze_params=config[\"freeze_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0521d916-d09c-4720-9727-f26c9cba0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model.inference(test_loader, out_dir=config[\"out_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61335525-88b6-4ad9-81d8-a5a58cf07b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_image_and_label(image_path, label_path, band_composite=(4,3,2), \n",
    "                         stretch=None, class_mapping=None):\n",
    "    \"\"\"\n",
    "    Plots a loaded image and its corresponding label using matplotlib and \n",
    "    rasterio.plot.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        label_path (str): Path to the label file.\n",
    "        stretch (str): A string specifying the contrast stretch to apply to the \n",
    "            image (e.g. 'linear', 'sqrt', or 'log').\n",
    "        band_composite (tuple of int): A tuple specifying the band indices to \n",
    "            use for an RGB image (e.g. (3, 2, 1)).\n",
    "        class_mapping (dict): A dictionary mapping class values to class names.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the image and label files\n",
    "    with rasterio.open(image_path) as src:\n",
    "        image_array = src.read()\n",
    "\n",
    "    with rasterio.open(label_path) as src:\n",
    "        label_array = src.read(1)\n",
    "\n",
    "    # Set up the figure and axes\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "\n",
    "    # Transpose the image_array if it's 3D and the first dimension is the smallest\n",
    "    if (image_array.ndim == 3) and (image_array.shape[0] == np.min(image_array.shape)):\n",
    "        image_array = image_array.transpose(1,2,0)\n",
    "    \n",
    "    image_size = [image_array.shape[0], image_array.shape[1]]\n",
    "    label_size = [label_array.shape[0], label_array.shape[1]]\n",
    "\n",
    "    # If a band composite is specified, create an RGB image\n",
    "    red_band = image_array[:, :, band_composite[0] - 1] / np.max(image_array[:, :, band_composite[0] - 1])\n",
    "    green_band = image_array[:, :, band_composite[1] - 1] / np.max(image_array[:, :, band_composite[1] - 1])\n",
    "    blue_band = image_array[:, :, band_composite[2] - 1] / np.max(image_array[:, :, band_composite[2] - 1])\n",
    "\n",
    "    # Stack the bands to create an RGB image\n",
    "    rgb_image = np.stack([red_band, green_band, blue_band], axis=-1)\n",
    "\n",
    "    # Apply the contrast stretch if specified\n",
    "    if stretch:\n",
    "        p_min, p_max = np.percentile(rgb_image, (2, 98))\n",
    "        stretched = exposure.rescale_intensity(\n",
    "            rgb_image, in_range=(p_min, p_max), out_range=(0, 1)\n",
    "        )\n",
    "    else:\n",
    "        stretched = rgb_image\n",
    "\n",
    "    # Display the RGB image using matplotlib\n",
    "    axs[0].imshow(stretched)\n",
    "    \n",
    "    # Add a title and axis labels\n",
    "    axs[0].set_title('Image')\n",
    "    axs[0].set_xlabel(f'# Column: {image_size[0]}')\n",
    "    axs[0].set_ylabel(f'# Row: {image_size[1]}')\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_aspect('equal')\n",
    "\n",
    "    # If class mapping is provided, create a colormap and a colorbar for the classes\n",
    "    if class_mapping is not None:\n",
    "        # Create a color map for your classes\n",
    "        cmap = mcolors.ListedColormap(['black', 'mediumseagreen', 'lime', 'forestgreen', 'yellow', \n",
    "                                        'yellowgreen', 'cyan', 'crimson', 'darkblue', 'wheat', \n",
    "                                        'goldenrod', 'gold', 'lightsteelblue', 'sandybrown', \n",
    "                                        'lightgray', 'darkorange', 'red'])\n",
    "\n",
    "    # Plot label chip\n",
    "    im = axs[1].imshow(label_array, cmap=cmap)\n",
    "    axs[1].set_title('Label')\n",
    "    axs[1].set_xlabel(f'# Column: {label_size[0]}')\n",
    "    axs[1].set_ylabel(f'# Row: {label_size[1]}')\n",
    "\n",
    "    # Create colorbar after imshow\n",
    "    if class_mapping is not None:\n",
    "        colorbar = fig.colorbar(im, ax=axs[1], ticks=list(class_mapping.keys()), label='class')\n",
    "        colorbar.ax.set_yticklabels(list(class_mapping.values()))  # set the colorbar labels to be the class names\n",
    "\n",
    "    # Show the plot\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_aspect('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0972919-0751-4364-9bdb-56337aeb169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/data/chips_filtered/chip_140_494_merged.tif\"\n",
    "label_path = \"/home/workdir/output2/hardened_prob/crisp_id_140_494.tif\"\n",
    "plot_image_and_label(image_path, label_path, band_composite=(4,3,2), stretch=\"linear\", class_mapping=config[\"class_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95637c60-b8f0-4ce3-9e09-2743506b2749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
